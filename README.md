# ğŸ¤– Multimodal Brain

> A comprehensive multimodal processing system that accepts YouTube links, documents, images, audio, and video files to build an intelligent knowledge base with AI-powered question answering.

## âœ¨ Features

- **ğŸ“º YouTube Processing**: Extract transcripts AND lyrics from YouTube videos
- **ğŸ“„ Document Processing**: PDF, DOCX, PPTX, MD, TXT files
- **ğŸ–¼ï¸ Image Analysis**: PNG, JPG image content analysis using Gemini Vision
- **ğŸµ Audio Transcription**: MP3 audio file transcription
- **ğŸ¬ Video Processing**: MP4 video content analysis and audio transcription
- **ğŸ” Semantic Search**: FAISS-powered vector database for intelligent content retrieval
- **ğŸ¤– AI-Powered Q&A**: Natural language question answering using Gemini AI
- **ğŸŒ Web Interface**: Clean, intuitive Streamlit UI

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Google Gemini API Key
- ffmpeg (for audio/video processing)

### Installation
```bash
# Clone repository
git clone https://github.com/TejaswarYambadi/Multi_Model_Brain.git
cd Multi_Model_Brain

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install streamlit google-genai PyPDF2 python-docx python-pptx Pillow pytube youtube-transcript-api SpeechRecognition pydub moviepy faiss-cpu scikit-learn numpy requests beautifulsoup4

# Set API key
echo GEMINI_API_KEY=your_api_key_here > .env

# Run application
streamlit run app.py --server.port 5000
```

## ğŸ“– Usage

### 1. Upload Files
- **Documents**: PDF, DOCX, PPTX, MD, TXT
- **Media**: PNG, JPG, MP3, MP4
- **YouTube**: Paste any YouTube video URL

### 2. Process Content
Click "ğŸ”„ Process Files" to extract and index content

### 3. Ask Questions
Enter natural language questions about your content:
- "What is the main topic discussed?"
- "Summarize the key points"
- "What are the lyrics about?"

## ğŸ§ª Test Cases for YouTube Links

### Test Case 1: Music Video with Lyrics
```
URL: https://www.youtube.com/watch?v=YQHsXMglC9A
Expected: Extract transcript + lyrics
Questions:
- "What is this song about?"
- "What are the main themes in the lyrics?"
- "Summarize the video content"
```

### Test Case 2: Educational Content
```
URL: https://www.youtube.com/watch?v=dQw4w9WgXcQ
Expected: Extract transcript only
Questions:
- "What is the main topic?"
- "List the key points mentioned"
- "Who is the speaker?"
```

### Test Case 3: No Transcript Available
```
URL: [Video without captions]
Expected: Metadata only
Questions:
- "What information is available?"
- "Describe the video title and channel"
```

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ app.py                    # Main Streamlit application
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ document_processor.py # PDF, DOCX, PPTX, TXT processing
â”‚   â”œâ”€â”€ image_processor.py    # Image analysis with Gemini Vision
â”‚   â”œâ”€â”€ audio_processor.py    # Audio transcription
â”‚   â”œâ”€â”€ video_processor.py    # Video processing
â”‚   â””â”€â”€ youtube_processor.py  # YouTube transcript + lyrics extraction
â”œâ”€â”€ database/
â”‚   â””â”€â”€ vector_db.py         # FAISS vector database
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ gemini_client.py     # Gemini AI client
â”‚   â””â”€â”€ file_utils.py        # File utilities
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Environment Variables
```bash
GEMINI_API_KEY=your_gemini_api_key_here
```

### Supported File Types
- **Documents**: .pdf, .docx, .pptx, .md, .txt
- **Images**: .png, .jpg, .jpeg
- **Audio**: .mp3
- **Video**: .mp4
- **YouTube**: Any valid YouTube video URL

## ğŸ§ª Testing

### Run Test Cases
```bash
# Test individual processors
python -c "from processors.youtube_processor import YouTubeProcessor; print(YouTubeProcessor().process('https://www.youtube.com/watch?v=YQHsXMglC9A'))"

# Test full application
streamlit run app.py --server.port 5000
```

### Expected Outputs
- **Transcript**: Spoken content from video
- **Lyrics**: Song lyrics (for music videos)
- **Metadata**: Title, channel, URL
- **AI Answers**: Context-aware responses

## ğŸš€ Deployment

### ğŸ’» Deploy on Another PC (Step-by-Step)

#### Step 1: System Requirements
```bash
# Check Python version (3.11+ required)
python --version

# Install Git (if not installed)
# Windows: Download from https://git-scm.com/
# Linux: sudo apt install git
# Mac: brew install git
```

#### Step 2: Install FFmpeg
```bash
# Windows (using chocolatey)
choco install ffmpeg

# Or download from: https://ffmpeg.org/download.html
# Extract and add to PATH

# Linux
sudo apt update
sudo apt install ffmpeg

# Mac
brew install ffmpeg
```

#### Step 3: Clone and Setup Project
```bash
# Clone the repository
git clone https://github.com/TejaswarYambadi/Multi_Model_Brain.git
cd Multi_Model_Brain

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

#### Step 4: Install Dependencies
```bash
# Install all required packages
pip install streamlit google-genai PyPDF2 python-docx python-pptx Pillow pytube youtube-transcript-api SpeechRecognition pydub moviepy faiss-cpu scikit-learn numpy requests beautifulsoup4

# Or create requirements.txt and install
pip freeze > requirements.txt
pip install -r requirements.txt
```

#### Step 5: Configure Environment
```bash
# Create .env file
echo GEMINI_API_KEY=your_actual_api_key_here > .env

# Or manually create .env file with:
# GEMINI_API_KEY=your_actual_api_key_here
```

#### Step 6: Test Installation
```bash
# Test individual components
python -c "import streamlit; print('Streamlit OK')"
python -c "from processors.youtube_processor import YouTubeProcessor; print('YouTube processor OK')"

# Test FFmpeg
ffmpeg -version
```

#### Step 7: Run Application
```bash
# Start the application
streamlit run app.py --server.port 5000

# Access via browser:
# http://localhost:5000
```

#### Step 8: Network Access (Optional)
```bash
# To access from other devices on network
streamlit run app.py --server.port 5000 --server.address 0.0.0.0

# Access via: http://YOUR_PC_IP:5000
```

### ğŸ³ Docker Deployment
```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy requirements and install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 5000

# Run application
CMD ["streamlit", "run", "app.py", "--server.port", "5000", "--server.address", "0.0.0.0"]
```

```bash
# Build and run with Docker
docker build -t multimodal-brain .
docker run -p 5000:5000 -e GEMINI_API_KEY=your_key multimodal-brain
```

### â˜ï¸ Cloud Deployment

#### Heroku
```bash
# Install Heroku CLI
# Create Procfile
echo "web: streamlit run app.py --server.port $PORT --server.address 0.0.0.0" > Procfile

# Deploy
heroku create your-app-name
heroku config:set GEMINI_API_KEY=your_key
git push heroku main
```

#### Railway/Render
```bash
# Create start command in platform:
streamlit run app.py --server.port $PORT --server.address 0.0.0.0

# Add environment variable:
GEMINI_API_KEY=your_key
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **Google Gemini**: AI-powered analysis and Q&A
- **Streamlit**: Web framework
- **FAISS**: Vector similarity search
- **Lyrics.ovh**: Lyrics API
- **YouTube Transcript API**: Transcript extraction

---

**Built with â¤ï¸ for Multimodal AI Processing**